# README

Heavily borrowed from [Fizz Buzz Tensorflow](https://github.com/joelgrus/fizz-buzz-tensorflow),
"enhanced" with some integration testing, multiclass prediction and the second fizz buzz feature "pop". 
An ML implementation of [the Fizz Buzz Kata](http://agilekatas.co.uk/katas/FizzBuzz-Kata). You can find the
implementation up to feature 1 on the `fizz_buzz` branch. The `master` branch contains an implementation of
feature 2 ("pop") as well.


You can use this as a sandbox for tensorflow to get a feeling for deep learning. 
What happens if...
 *  you add more data (Increase `num_digits`)?
 *  you increase the depth of the network (Increase `num_hidden`)?
 *  you allow the network to learn for longer (Increase `num_epochs`)?
 *  you change the `learning_rate`?
 *  you change the `batch_size`?
 *  you change the optimisation method?
 *  you increase the complexity of the model by adding more or different layers in?
